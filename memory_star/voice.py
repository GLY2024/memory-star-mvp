"""
è®°å¿†æ˜Ÿæ²³ - è¯­éŸ³å¤„ç†æ¨¡å—
æ”¯æŒGPT-4o Realtime APIå’ŒGemini Live API
é€‚é…ç”µè„‘ç«¯å’Œæ‰‹æœºç«¯ä¸åŒåœºæ™¯
"""

import os
import io
import base64
import tempfile
from abc import ABC, abstractmethod
from typing import Optional, Callable, AsyncIterator
from dataclasses import dataclass
from enum import Enum


class VoiceProvider(Enum):
    """è¯­éŸ³æœåŠ¡æä¾›å•†"""
    OPENAI = "openai"      # GPT-4o Realtime API
    GEMINI = "gemini"      # Gemini Live API
    MOCK = "mock"          # æ¨¡æ‹Ÿæ¨¡å¼ï¼ˆæ–‡å­—æ›¿ä»£ï¼‰


@dataclass
class VoiceConfig:
    """è¯­éŸ³é…ç½®"""
    provider: VoiceProvider
    api_key: Optional[str] = None
    model: Optional[str] = None
    voice: str = "alloy"   # OpenAI: alloy, echo, fable, onyx, nova, shimmer
    language: str = "zh"   # è¯­è¨€ä»£ç 
    
    # éŸ³é¢‘å‚æ•°
    sample_rate: int = 24000
    channels: int = 1
    
    # ç§»åŠ¨ç«¯ç‰¹æœ‰é…ç½®
    enable_vad: bool = True        # è¯­éŸ³æ´»åŠ¨æ£€æµ‹
    vad_threshold: float = 0.5     # VADé˜ˆå€¼
    
    @classmethod
    def from_env(cls) -> "VoiceConfig":
        """ä»ç¯å¢ƒå˜é‡åˆ›å»ºé…ç½®"""
        provider_str = os.getenv("VOICE_PROVIDER", "mock").lower()
        
        if provider_str == "openai":
            provider = VoiceProvider.OPENAI
            api_key = os.getenv("OPENAI_API_KEY")
            model = "gpt-4o-realtime-preview"
        elif provider_str == "gemini":
            provider = VoiceProvider.GEMINI
            api_key = os.getenv("GEMINI_API_KEY")
            model = "gemini-2.0-flash-exp"
        else:
            provider = VoiceProvider.MOCK
            api_key = None
            model = None
        
        return cls(
            provider=provider,
            api_key=api_key,
            model=model,
            voice=os.getenv("VOICE_NAME", "alloy"),
            language=os.getenv("VOICE_LANGUAGE", "zh"),
        )


class BaseVoiceHandler(ABC):
    """è¯­éŸ³å¤„ç†å™¨åŸºç±»"""
    
    def __init__(self, config: VoiceConfig):
        self.config = config
        self.is_listening = False
        self.is_speaking = False
    
    @abstractmethod
    async def connect(self):
        """å»ºç«‹è¿æ¥"""
        pass
    
    @abstractmethod
    async def disconnect(self):
        """æ–­å¼€è¿æ¥"""
        pass
    
    @abstractmethod
    async def send_audio(self, audio_data: bytes) -> str:
        """
        å‘é€éŸ³é¢‘æ•°æ®ï¼Œè¿”å›AIæ–‡æœ¬å›å¤
        ç”µè„‘ç«¯ï¼šä»éº¦å…‹é£è¯»å–
        æ‰‹æœºç«¯ï¼šä»æ‰‹æœºéº¦å…‹é£è¯»å–
        """
        pass
    
    @abstractmethod
    async def speak(self, text: str) -> bytes:
        """
        å°†æ–‡æœ¬è½¬ä¸ºè¯­éŸ³
        è¿”å›éŸ³é¢‘æ•°æ®ï¼ˆPCMæˆ–MP3ï¼‰
        """
        pass
    
    @abstractmethod
    async def stream_conversation(
        self, 
        on_text: Callable[[str], None],
        on_audio: Callable[[bytes], None]
    ):
        """
        æµå¼å¯¹è¯ï¼ˆRealtimeæ¨¡å¼ï¼‰
        æŒç»­ç›‘å¬éŸ³é¢‘è¾“å…¥ï¼Œå®æ—¶è¾“å‡ºè¯­éŸ³å›å¤
        """
        pass


class OpenAIVoiceHandler(BaseVoiceHandler):
    """OpenAI GPT-4o Realtime API è¯­éŸ³å¤„ç†å™¨"""
    
    def __init__(self, config: VoiceConfig):
        super().__init__(config)
        self.ws = None
        self.client = None
        
    async def connect(self):
        """å»ºç«‹WebSocketè¿æ¥"""
        import websockets
        import json
        
        if not self.config.api_key:
            raise ValueError("OpenAI API key not configured")
        
        url = f"wss://api.openai.com/v1/realtime?model={self.config.model}"
        headers = {
            "Authorization": f"Bearer {self.config.api_key}",
            "OpenAI-Beta": "realtime=v1"
        }
        
        self.ws = await websockets.connect(url, extra_headers=headers)
        
        # é…ç½®ä¼šè¯
        config_msg = {
            "type": "session.update",
            "session": {
                "modalities": ["text", "audio"],
                "instructions": "ä½ æ˜¯ä¸€ä½æ¸©æš–çš„å›å¿†å½•è®¿è°ˆåŠ©æ‰‹ï¼Œç”¨äº²åˆ‡çš„è¯­æ°”ä¸è€äººäº¤è°ˆã€‚",
                "voice": self.config.voice,
                "input_audio_format": "pcm16",
                "output_audio_format": "pcm16",
                "input_audio_transcription": {
                    "model": "whisper-1"
                },
                "turn_detection": {
                    "type": "server_vad",
                    "threshold": self.config.vad_threshold,
                    "prefix_padding_ms": 300,
                    "silence_duration_ms": 500
                }
            }
        }
        await self.ws.send(json.dumps(config_msg))
    
    async def disconnect(self):
        """æ–­å¼€è¿æ¥"""
        if self.ws:
            await self.ws.close()
            self.ws = None
    
    async def send_audio(self, audio_data: bytes) -> str:
        """å‘é€éŸ³é¢‘å¹¶è·å–å›å¤"""
        import json
        import base64
        
        if not self.ws:
            await self.connect()
        
        # å‘é€éŸ³é¢‘æ•°æ®
        audio_msg = {
            "type": "input_audio_buffer.append",
            "audio": base64.b64encode(audio_data).decode()
        }
        await self.ws.send(json.dumps(audio_msg))
        
        # æäº¤éŸ³é¢‘
        await self.ws.send(json.dumps({"type": "input_audio_buffer.commit"}))
        await self.ws.send(json.dumps({"type": "response.create"}))
        
        # æ¥æ”¶å›å¤
        transcript = ""
        async for message in self.ws:
            data = json.loads(message)
            
            if data["type"] == "response.text.delta":
                transcript += data.get("delta", "")
            elif data["type"] == "response.done":
                break
        
        return transcript
    
    async def speak(self, text: str) -> bytes:
        """æ–‡æœ¬è½¬è¯­éŸ³"""
        from openai import OpenAI
        
        client = OpenAI(api_key=self.config.api_key)
        
        response = client.audio.speech.create(
            model="tts-1",
            voice=self.config.voice,
            input=text,
            response_format="pcm"  # è¿”å›PCMæ ¼å¼ï¼Œä¾¿äºæµå¼æ’­æ”¾
        )
        
        return response.content
    
    async def stream_conversation(
        self,
        on_text: Callable[[str], None],
        on_audio: Callable[[bytes], None]
    ):
        """æµå¼å¯¹è¯ï¼ˆRealtimeæ¨¡å¼ï¼‰"""
        import json
        import base64
        
        if not self.ws:
            await self.connect()
        
        self.is_listening = True
        
        try:
            async for message in self.ws:
                data = json.loads(message)
                msg_type = data.get("type")
                
                # å¤„ç†ç”¨æˆ·è¯­éŸ³è½¬å½•
                if msg_type == "conversation.item.input_audio_transcription.completed":
                    text = data.get("transcript", "")
                    on_text(f"[ç”¨æˆ·] {text}")
                
                # å¤„ç†AIæ–‡æœ¬å›å¤
                elif msg_type == "response.text.delta":
                    delta = data.get("delta", "")
                    on_text(delta)
                
                # å¤„ç†AIè¯­éŸ³å›å¤
                elif msg_type == "response.audio.delta":
                    audio = base64.b64decode(data.get("delta", ""))
                    on_audio(audio)
                
                # å›å¤å®Œæˆ
                elif msg_type == "response.done":
                    on_text("\n")
                    
        except Exception as e:
            print(f"Stream error: {e}")
        finally:
            self.is_listening = False


class GeminiVoiceHandler(BaseVoiceHandler):
    """Gemini Live API è¯­éŸ³å¤„ç†å™¨"""
    
    def __init__(self, config: VoiceConfig):
        super().__init__(config)
        self.session = None
    
    async def connect(self):
        """å»ºç«‹è¿æ¥"""
        import google.generativeai as genai
        
        if not self.config.api_key:
            raise ValueError("Gemini API key not configured")
        
        genai.configure(api_key=self.config.api_key)
        
        model = genai.GenerativeModel(
            model_name=self.config.model or "gemini-2.0-flash-exp",
            generation_config={
                "temperature": 0.8,
                "max_output_tokens": 2048,
            }
        )
        
        # å¯åŠ¨è¯­éŸ³ä¼šè¯
        self.session = model.start_chat()
    
    async def disconnect(self):
        """æ–­å¼€è¿æ¥"""
        self.session = None
    
    async def send_audio(self, audio_data: bytes) -> str:
        """å‘é€éŸ³é¢‘å¹¶è·å–å›å¤"""
        import google.generativeai as genai
        
        if not self.session:
            await self.connect()
        
        # Geminiæ”¯æŒç›´æ¥å‘é€éŸ³é¢‘
        audio_part = {
            "mime_type": "audio/pcm",
            "data": audio_data
        }
        
        response = self.session.send_message(audio_part)
        return response.text
    
    async def speak(self, text: str) -> bytes:
        """Geminiæš‚ä¸æ”¯æŒç›´æ¥TTSï¼Œä½¿ç”¨OpenAI TTSä½œä¸ºfallback"""
        # å¯ä»¥é›†æˆå…¶ä»–TTSæœåŠ¡
        raise NotImplementedError("Gemini TTS not implemented, use OpenAI TTS instead")
    
    async def stream_conversation(self, on_text, on_audio):
        """Gemini Liveæµå¼å¯¹è¯"""
        # Gemini Live APIå®ç°
        pass


class MockVoiceHandler(BaseVoiceHandler):
    """æ¨¡æ‹Ÿè¯­éŸ³å¤„ç†å™¨ï¼ˆæ–‡å­—æ¨¡å¼ï¼‰"""
    
    async def connect(self):
        pass
    
    async def disconnect(self):
        pass
    
    async def send_audio(self, audio_data: bytes) -> str:
        """æ¨¡æ‹Ÿï¼šè¿”å›æç¤ºè®©ç”¨æˆ·è¾“å…¥æ–‡å­—"""
        return "[è¯­éŸ³æ¨¡å¼æœªå¯ç”¨ï¼Œè¯·è¾“å…¥æ–‡å­—]"
    
    async def speak(self, text: str) -> bytes:
        """æ¨¡æ‹Ÿï¼šä»…æ‰“å°æ–‡å­—"""
        print(f"[è¯­éŸ³è¾“å‡º] {text}")
        return b""
    
    async def stream_conversation(self, on_text, on_audio):
        pass


def create_voice_handler(config: Optional[VoiceConfig] = None) -> BaseVoiceHandler:
    """å·¥å‚å‡½æ•°ï¼šåˆ›å»ºè¯­éŸ³å¤„ç†å™¨"""
    config = config or VoiceConfig.from_env()
    
    if config.provider == VoiceProvider.OPENAI:
        return OpenAIVoiceHandler(config)
    elif config.provider == VoiceProvider.GEMINI:
        return GeminiVoiceHandler(config)
    else:
        return MockVoiceHandler(config)


# ==================== å¹³å°é€‚é…å±‚ ====================

class PlatformAdapter(ABC):
    """
    å¹³å°é€‚é…å™¨æŠ½è±¡åŸºç±»
    ç”µè„‘ç«¯å’Œæ‰‹æœºç«¯æœ‰ä¸åŒçš„éŸ³é¢‘é‡‡é›†å’Œæ’­æ”¾æ–¹å¼
    """
    
    @abstractmethod
    async def record_audio(self, duration: Optional[float] = None) -> bytes:
        """å½•åˆ¶éŸ³é¢‘"""
        pass
    
    @abstractmethod
    async def play_audio(self, audio_data: bytes):
        """æ’­æ”¾éŸ³é¢‘"""
        pass
    
    @abstractmethod
    def is_mobile(self) -> bool:
        """æ˜¯å¦ä¸ºç§»åŠ¨ç«¯"""
        pass


class DesktopAdapter(PlatformAdapter):
    """ç”µè„‘ç«¯é€‚é…å™¨ï¼ˆLinux/Mac/Windowsï¼‰"""
    
    def __init__(self):
        self.sample_rate = 24000
        self.channels = 1
        self.dtype = "int16"
    
    async def record_audio(self, duration: Optional[float] = None) -> bytes:
        """
        ä½¿ç”¨sounddeviceå½•åˆ¶éŸ³é¢‘
        ç”µè„‘ç«¯ï¼šæ”¯æŒé•¿æ—¶é—´å½•åˆ¶ï¼Œæœ‰åœæ­¢æŒ‰é’®
        """
        import sounddevice as sd
        import numpy as np
        
        if duration:
            # å›ºå®šæ—¶é•¿å½•åˆ¶
            recording = sd.rec(
                int(duration * self.sample_rate),
                samplerate=self.sample_rate,
                channels=self.channels,
                dtype=self.dtype
            )
            sd.wait()
            return recording.tobytes()
        else:
            # æŒç»­å½•åˆ¶ç›´åˆ°æ‰‹åŠ¨åœæ­¢ï¼ˆéœ€è¦UIé…åˆï¼‰
            # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå½•åˆ¶5ç§’
            return await self.record_audio(duration=5.0)
    
    async def play_audio(self, audio_data: bytes):
        """æ’­æ”¾éŸ³é¢‘"""
        import sounddevice as sd
        import numpy as np
        
        audio_array = np.frombuffer(audio_data, dtype=np.int16)
        sd.play(audio_array, self.sample_rate)
        sd.wait()
    
    def is_mobile(self) -> bool:
        return False


class MobileAdapter(PlatformAdapter):
    """
    æ‰‹æœºç«¯é€‚é…å™¨
    æ³¨æ„ï¼šè¿™æ˜¯æ¥å£å®šä¹‰ï¼Œå®é™…å®ç°éœ€è¦åœ¨åŸç”ŸAppä¸­
    """
    
    async def record_audio(self, duration: Optional[float] = None) -> bytes:
        """
        æ‰‹æœºç«¯éŸ³é¢‘é‡‡é›†æ–¹å¼ï¼š
        - iOS: AVAudioRecorder
        - Android: MediaRecorder / AudioRecord
        - React Native: expo-av / react-native-audio-recorder-player
        """
        raise NotImplementedError(
            "Mobile audio recording should be implemented in native app"
        )
    
    async def play_audio(self, audio_data: bytes):
        """
        æ‰‹æœºç«¯éŸ³é¢‘æ’­æ”¾æ–¹å¼ï¼š
        - iOS: AVAudioPlayer
        - Android: MediaPlayer / AudioTrack
        """
        raise NotImplementedError(
            "Mobile audio playback should be implemented in native app"
        )
    
    def is_mobile(self) -> bool:
        return True


# ==================== ç»Ÿä¸€è¯­éŸ³æ¥å£ ====================

class VoiceInterface:
    """
    ç»Ÿä¸€è¯­éŸ³æ¥å£
    æ ¹æ®å¹³å°è‡ªåŠ¨é€‰æ‹©é€‚é…å™¨ï¼Œæä¾›ä¸€è‡´çš„è¯­éŸ³äº¤äº’ä½“éªŒ
    """
    
    def __init__(
        self,
        voice_handler: Optional[BaseVoiceHandler] = None,
        platform_adapter: Optional[PlatformAdapter] = None
    ):
        self.voice = voice_handler or create_voice_handler()
        self.platform = platform_adapter or self._detect_platform()
        self.conversation_history = []
    
    def _detect_platform(self) -> PlatformAdapter:
        """è‡ªåŠ¨æ£€æµ‹å¹³å°"""
        # ç®€å•æ£€æµ‹ï¼šæ£€æŸ¥æ˜¯å¦åœ¨ç§»åŠ¨ç¯å¢ƒ
        # å®é™…é¡¹ç›®ä¸­å¯ä»¥é€šè¿‡user agentæˆ–å…¶ä»–æ–¹å¼æ£€æµ‹
        return DesktopAdapter()
    
    async def start_conversation(self):
        """å¼€å§‹è¯­éŸ³å¯¹è¯"""
        await self.voice.connect()
        print("ğŸ™ï¸ è¯­éŸ³å¯¹è¯å·²å¯åŠ¨ï¼Œè¯·è¯´è¯...")
    
    async def stop_conversation(self):
        """åœæ­¢è¯­éŸ³å¯¹è¯"""
        await self.voice.disconnect()
        print("ğŸ‘‹ è¯­éŸ³å¯¹è¯å·²ç»“æŸ")
    
    async def speak_turn(self, prompt_text: Optional[str] = None) -> str:
        """
        ä¸€è½®è¯­éŸ³å¯¹è¯ï¼š
        1. æ’­æ”¾æç¤ºéŸ³ï¼ˆå¯é€‰ï¼‰
        2. å½•åˆ¶ç”¨æˆ·è¯­éŸ³
        3. å‘é€åˆ°AI
        4. æ’­æ”¾AIå›å¤
        5. è¿”å›è½¬å½•æ–‡æœ¬
        """
        # æ’­æ”¾æç¤ºéŸ³
        if prompt_text:
            await self.voice.speak(prompt_text)
        
        print("ğŸ¤ è¯·è¯´è¯...")
        
        # å½•åˆ¶éŸ³é¢‘
        audio_input = await self.platform.record_audio(duration=10.0)
        
        print("ğŸ¤” æ€è€ƒä¸­...")
        
        # å‘é€åˆ°AI
        response_text = await self.voice.send_audio(audio_input)
        
        # æ’­æ”¾å›å¤
        response_audio = await self.voice.speak(response_text)
        await self.platform.play_audio(response_audio)
        
        # è®°å½•å†å²
        self.conversation_history.append({
            "user_audio": audio_input,
            "ai_text": response_text,
            "ai_audio": response_audio
        })
        
        return response_text
    
    async def continuous_chat(self, max_turns: int = 10):
        """
        è¿ç»­å¤šè½®å¯¹è¯
        é€‚åˆç”µè„‘ç«¯æ¼”ç¤ºä½¿ç”¨
        """
        await self.start_conversation()
        
        try:
            for i in range(max_turns):
                print(f"\n--- ç¬¬ {i+1} è½® ---")
                response = await self.speak_turn()
                print(f"AI: {response}")
                
                # æ£€æŸ¥æ˜¯å¦ç»“æŸ
                if "å†è§" in response or "ç»“æŸ" in response:
                    break
        finally:
            await self.stop_conversation()
